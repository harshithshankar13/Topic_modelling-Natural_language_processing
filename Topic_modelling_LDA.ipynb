{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "case_studies_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1mjAVUzb1FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install package\n",
        "pip install GetOldTweets3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFJXXWGkkXC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install package\n",
        "pip install pyLDAvis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQvtTY7kdGYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import \n",
        "import GetOldTweets3 as getOTw3\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybEHi1IXdM2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tweets from the galwaybayfmnews using GetOldTweets3\n",
        "galwaybayfmTweetObj = getOTw3.manager.TweetCriteria().setQuerySearch('Galwaybayfmnews')\\\n",
        "                                           .setSince(\"2019-08-7\")\\\n",
        "                                           .setUntil(\"2020-04-7\")\n",
        "\n",
        "# get tweets \n",
        "galwayBayFmTweets = getOTw3.manager.TweetManager.getTweets(galwaybayfmTweetObj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sai0E_LCent1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe from imported tweets\n",
        "galwayBay_df = pd.DataFrame()\n",
        "\n",
        "# store tweet into data frame\n",
        "for tweet in galwayBayFmTweets:\n",
        "   galwayBay_df = galwayBay_df.append(pd.Series(tweet.text), ignore_index=True)\n",
        "   print(tweet.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCIAYy5mSzp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "# remove all the punctuation in the tweets\n",
        "galwayBay_df['process'] = galwayBay_df[0].map(lambda x: re.sub('[,\\\\.!?]', '', x))\n",
        "# split string into array by space\n",
        "galwayBay_df['process'] = galwayBay_df['process'].str.lower().str.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNgPx4zdTQQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build list of words from data frame\n",
        "words = []\n",
        "for word in galwayBay_df.loc[:,'process']:\n",
        "  for sword in word:\n",
        "    words.append(sword)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oVR9sr6TqF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "# define prefix words to remove from list of words\n",
        "prefixes = ('http', 'https', 'news', 'galway','uhg','oughterard','new','says','it', 'galwayad','know','connemara', 'tuam' ,'we' ,'one', 'us','say','go','thanks','td','well','next','back','loughrea', 'see', 're', '&amp','may','ve','like','johnson', 'three' ,'two','near','that','na','ugh','galwayad','boris','galwaycoco','fyi','12','please','help')\n",
        "# define suffix words to remove from list of words\n",
        "suffix = ('galway', 'galwaybayfmnews', 'news',  'galwayad', '&amp')\n",
        "# remove prefix and suffix\n",
        "words = [x for x in words if not x.startswith(prefixes)]\n",
        "words = [x for x in words if not x.endswith(suffix)]\n",
        "\n",
        "# remove stopwords\n",
        "words = [item for item in words if item not in ENGLISH_STOP_WORDS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k_YLA4he14i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to get matrix of token counts\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create tokenizer\n",
        "countVectObj = CountVectorizer()\n",
        "\n",
        "# get matrix of token counts of imported tweets\n",
        "tokenData = countVectObj.fit_transform(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdkYuVbjfgq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import LDA model from scikit learn \n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# total number of topics\n",
        "num_of_topic = 20\n",
        "\n",
        "# create lda model with 50 iterations and learning method is online\n",
        "lda_model = LatentDirichletAllocation(n_components=num_of_topic, max_iter=50, learning_method='online')\n",
        "lda_topics = lda_model.fit_transform(tokenData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN2EHr66i8gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualise the topics in the cluster\n",
        "def Visulise_topics(lda, features, numOfword):\n",
        "    for top_idx, topic in enumerate(lda.components_):\n",
        "        print(\"Topic %d:\" % (top_idx))\n",
        "        print(\" \".join([features[i] for i in topic.argsort()[:-numOfword - 1:-1]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jpvA4q3jR5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get feature names from tokenizer\n",
        "features = countVectObj.get_feature_names()\n",
        "# visulise the topics\n",
        "Visulise_topics(lda_model, features, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpz1PRvGf-nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove the irrelevant posts from the model's output\n",
        "topicsPerDocu = pd.DataFrame(lda_topics, columns=[\"Topic\"+str(i+1) for i in range(num_of_topic)])\n",
        "\n",
        "# to compare all the topic probablity is equal in a document\n",
        "val = 1/num_of_topic\n",
        "# remove rows having all the topic probablities are equal\n",
        "topicsPerDocu = topicsPerDocu.loc[(topicsPerDocu != val).all(axis=1),]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd59lAXlhkqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select a topic with high probability as a topic of the perticular document\n",
        "highProbTopic = topicsPerDocu.idxmax(axis=1)\n",
        "# count number of documents per topic\n",
        "docCountPerTopic = highProbTopic.groupby(highProbTopic).count()\n",
        "\n",
        "# convert series into dataframe\n",
        "docCountPerTopic_df = pd.DataFrame(docCountPerTopic)\n",
        "docCountPerTopic_df.reset_index( inplace=True)\n",
        "docCountPerTopic_df.columns = [\"Topic Name\", 'Number of Tweets']\n",
        "\n",
        "# sort the dataframe in ascending order of the number of tweets\n",
        "docCountPerTopic_df.sort_values(by=['Number of Tweets'], inplace=True)\n",
        "\n",
        "# By manual selecting topics topics 1,3,13,15,17,8\n",
        "docCountPerTopic_df = docCountPerTopic_df.loc[docCountPerTopic_df['Topic Name'].isin(['Topic1','Topic3','Topic8','Topic13','Topic15','Topic17'])]\n",
        "\n",
        "docCountPerTopic_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHy7XizjUfRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save table to csv\n",
        "docCountPerTopic_df.to_csv(\"tweetsPerTopic.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKeD20gair4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualise document per topics\n",
        "# bar plot\n",
        "import matplotlib.pyplot as plot\n",
        "plot.style.use('ggplot')\n",
        "\n",
        "# define figure object\n",
        "figure = plot.figure()\n",
        "# define axes object\n",
        "axes = figure.add_axes([0,0,1,1])\n",
        "\n",
        "# visulise bar chart\n",
        "plot.bar(docCountPerTopic_df['Topic Name'], docCountPerTopic_df['Number of Tweets'], width=0.4, color=['#009E73', \"#CC79A7\", \"#E69F00\", \"#000000\"])\n",
        "\n",
        "# set title and ylabel\n",
        "plot.ylabel('Number of tweets per topic')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4o1Y_4hjjmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to visualise the LDA topic model using pyLDAvis \n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(lda_model, tokenData, countVectObj, mds='tsne', R=5)\n",
        "panel"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}